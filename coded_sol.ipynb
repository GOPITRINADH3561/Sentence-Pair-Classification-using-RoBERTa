{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7d6a6a4-f2a6-465d-8074-d7f39c79773f",
   "metadata": {},
   "source": [
    "<h1>Assignment 5</h1>\n",
    "<h3><b>Name: </b>Gopi Trinadh Maddikunta</h3>\n",
    "<h3><b>PSID: </b>2409404</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c00db651-8cb3-4e36-babc-561b3671f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers datasets evaluate accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c3230b-7a23-41a0-9d5c-1725c1267557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Import Libraries ---\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW  # Correct: from torch.optim\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_scheduler\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3462afb-b72d-435d-aff5-72471ca208db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configurations ---\n",
    "model_checkpoint = \"roberta-base\"\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "num_epochs_frozen = 15   # 10-20 epochs\n",
    "num_epochs_unfrozen = 4  # 3-5 epochs\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3207a6-2076-4191-b7f4-5117fd4cac2e",
   "metadata": {},
   "source": [
    "<h3>Task 1: Paraphrase Detection using RoBERTa</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4af3f31c-c216-4f69-9fb9-78ec03c8433c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Dataset ---\n",
    "dataset = load_dataset(\"glue\", \"mrpc\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding=\"max_length\")\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99e52916-6d63-4ca7-8ac8-b702c3e627de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Dataloaders ---\n",
    "train_loader = DataLoader(encoded_dataset['train'], batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(encoded_dataset['validation'], batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcb21d38-ae85-486f-8f9d-16e6d4e3083d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Early Stopping ---\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c0d011e-c35a-4c2d-977e-e25cddfa18a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- LoRA Preparation ---\n",
    "# !pip install peft  # Uncomment if PEFT is not installed\n",
    "\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "def apply_lora(model):\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.SEQ_CLS,\n",
    "        r=8,           \n",
    "        lora_alpha=32,\n",
    "        lora_dropout=0.1,\n",
    "        target_modules=[\"query\", \"key\", \"value\"]\n",
    "    )\n",
    "    model = get_peft_model(model, lora_config)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a96d0c07-a6e2-4e9e-87cb-53e8428fc124",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.65it/s, loss=0.646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6254, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.70it/s, loss=0.635]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6230, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.69it/s, loss=0.633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6221, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.69it/s, loss=0.628]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6196, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.68it/s, loss=0.629]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6179, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.68it/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6180, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.67it/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6164, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.67it/s, loss=0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6161, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.67it/s, loss=0.623]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6150, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.66it/s, loss=0.622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6148, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.66it/s, loss=0.622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6146, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.70it/s, loss=0.621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6140, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.70it/s, loss=0.619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6138, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.70it/s, loss=0.619]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6136, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/15 (Frozen): 100%|██████████| 115/115 [00:24<00:00,  4.70it/s, loss=0.62] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6135, Accuracy: 0.6838, Precision: 0.6838, Recall: 1.0000, F1: 0.8122\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Part 1: Fine-tune with Frozen RoBERTa Base + Early Stopping\n",
    "\n",
    "# --- Start of Part 1: Frozen Training (with Early Stopping) ---\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Freeze all layers except classification head\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = len(train_loader) * num_epochs_frozen\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(num_epochs_frozen):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs_frozen} (Frozen)\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        # Explicitly pass fields\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"]\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=total_loss / len(train_loader))\n",
    "\n",
    "    # Validation after each epoch\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds, labels = [], []\n",
    "    for batch in valid_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"label\"]\n",
    "            )\n",
    "        logits = outputs.logits\n",
    "        val_loss += outputs.loss.item()\n",
    "        preds.extend(torch.argmax(logits, axis=-1).cpu().numpy())\n",
    "        labels.extend(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    avg_val_loss = val_loss / len(valid_loader)\n",
    "    print(f\"Validation --> Loss: {avg_val_loss:.4f}, Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# Save Frozen Model\n",
    "torch.save(model.state_dict(), \"roberta_mrpc_frozen.pt\")\n",
    "# --- End of Part 1: Frozen Training (with Early Stopping) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf5214c8-c124-4ad2-a0d5-0e401034cb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4 (Unfrozen): 100%|██████████| 115/115 [01:11<00:00,  1.62it/s, loss=0.535]\n",
      "Epoch 2/4 (Unfrozen): 100%|██████████| 115/115 [01:11<00:00,  1.62it/s, loss=0.349]\n",
      "Epoch 3/4 (Unfrozen): 100%|██████████| 115/115 [01:11<00:00,  1.62it/s, loss=0.235]\n",
      "Epoch 4/4 (Unfrozen): 100%|██████████| 115/115 [01:11<00:00,  1.62it/s, loss=0.155]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Validation --> Accuracy: 0.8775, Precision: 0.8908, Recall: 0.9355, F1: 0.9126\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# 🚀 Part 2: Fine-tune with Unfrozen RoBERTa Base (Optional LoRA)\n",
    "# ======================================================\n",
    "# --- Start of Part 2: Unfrozen Training ---\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# OPTIONAL: Apply LoRA if training is slow\n",
    "# model = apply_lora(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = len(train_loader) * num_epochs_unfrozen\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(num_epochs_unfrozen):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs_unfrozen} (Unfrozen)\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"]\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=total_loss / len(train_loader))\n",
    "\n",
    "# Final evaluation on validation set\n",
    "model.eval()\n",
    "preds, labels = [], []\n",
    "for batch in valid_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"]\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    preds.extend(torch.argmax(logits, axis=-1).cpu().numpy())\n",
    "    labels.extend(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "acc = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "print(f\"Final Validation --> Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "# Save Unfrozen Model\n",
    "torch.save(model.state_dict(), \"roberta_mrpc_unfrozen.pt\")\n",
    "# --- End of Part 2: Unfrozen Training ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4cd659-beda-4ff5-a8df-fddf849fabc9",
   "metadata": {},
   "source": [
    "<h3>Task 2: Entailment Detection using RoBERTa:</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4f37bfb-c1e5-40e7-bd86-92db969ed1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Dataset\n",
    "dataset = load_dataset(\"glue\", \"rte\")\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding=\"max_length\")\n",
    "\n",
    "encoded_dataset = dataset.map(tokenize_function, batched=True)\n",
    "encoded_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95609d0d-c324-4722-9428-d57703ada5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoaders\n",
    "# Split train 80-20\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "train_size = int(0.8 * len(encoded_dataset['train']))\n",
    "val_size = len(encoded_dataset['train']) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(encoded_dataset['train'], [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(encoded_dataset['validation'], batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "461d90e6-403f-446f-b3c0-eef959e5fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early Stopping Utility\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=3, min_delta=0):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss - self.min_delta:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "69a7054a-7b75-470c-bfc7-735a9e78e6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/15 (Frozen): 100%|██████████| 63/63 [00:13<00:00,  4.75it/s, loss=0.695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6926, Accuracy: 0.4980, Precision: 0.5011, Recall: 0.9084, F1: 0.6459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 (Frozen): 100%|██████████| 63/63 [00:13<00:00,  4.73it/s, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6927, Accuracy: 0.5201, Precision: 0.5469, Recall: 0.2789, F1: 0.3694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 (Frozen): 100%|██████████| 63/63 [00:13<00:00,  4.72it/s, loss=0.696]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6924, Accuracy: 0.5040, Precision: 0.5040, Recall: 1.0000, F1: 0.6702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 (Frozen): 100%|██████████| 63/63 [00:13<00:00,  4.69it/s, loss=0.694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation --> Loss: 0.6923, Accuracy: 0.5060, Precision: 0.5052, Recall: 0.9602, F1: 0.6621\n",
      "Early stopping triggered!\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "# Part 1: Fine-tune with Frozen RoBERTa Base + Early Stopping\n",
    "# ======================================================\n",
    "# --- Start of Part 1: Frozen Training (with Early Stopping) ---\n",
    "\n",
    "early_stopping = EarlyStopping(patience=3, min_delta=0.001)\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# Freeze all layers except classification head\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = len(train_loader) * num_epochs_frozen\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(num_epochs_frozen):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs_frozen} (Frozen)\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"]\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=total_loss / len(train_loader))\n",
    "\n",
    "    # Validation (on 20% split)\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    preds, labels = [], []\n",
    "    for batch in val_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        with torch.no_grad():\n",
    "            outputs = model(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"label\"]\n",
    "            )\n",
    "        logits = outputs.logits\n",
    "        val_loss += outputs.loss.item()\n",
    "        preds.extend(torch.argmax(logits, axis=-1).cpu().numpy())\n",
    "        labels.extend(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    print(f\"Validation --> Loss: {avg_val_loss:.4f}, Accuracy: {acc:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1: {f1:.4f}\")\n",
    "\n",
    "    early_stopping(avg_val_loss)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered!\")\n",
    "        break\n",
    "\n",
    "# Save Frozen Model\n",
    "torch.save(model.state_dict(), \"roberta_rte_frozen.pt\")\n",
    "# --- End of Part 1: Frozen Training (with Early Stopping) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b06d687e-ea0c-4c78-af00-576dcc57cb93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Epoch 1/4 (Unfrozen): 100%|██████████| 63/63 [00:38<00:00,  1.64it/s, loss=0.694]\n",
      "Epoch 2/4 (Unfrozen): 100%|██████████| 63/63 [00:38<00:00,  1.63it/s, loss=0.637]\n",
      "Epoch 3/4 (Unfrozen): 100%|██████████| 63/63 [00:38<00:00,  1.63it/s, loss=0.492] \n",
      "Epoch 4/4 (Unfrozen): 100%|██████████| 63/63 [00:38<00:00,  1.62it/s, loss=0.366] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Unfrozen Model - Final Test Results on RTE Validation Set (277 samples)\n",
      "Accuracy: 0.7148\n",
      "Precision: 0.7955\n",
      "Recall: 0.5344\n",
      "F1 Score: 0.6393\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "#  Part 2: Fine-tune with Unfrozen RoBERTa Base (Optional LoRA)\n",
    "# ======================================================\n",
    "# --- Start of Part 2: Unfrozen Training ---\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "model.to(device)\n",
    "\n",
    "# OPTIONAL: Apply LoRA if training is slow\n",
    "# model = apply_lora(model)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = len(train_loader) * num_epochs_unfrozen\n",
    "lr_scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "for epoch in range(num_epochs_unfrozen):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs_unfrozen} (Unfrozen)\")\n",
    "    for batch in progress_bar:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"]\n",
    "        )\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=total_loss / len(train_loader))\n",
    "\n",
    "# Final evaluation on validation set (277 samples)\n",
    "model.eval()\n",
    "preds, labels = [], []\n",
    "\n",
    "# Evaluate on full RTE validation split (277 examples)\n",
    "for batch in test_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"]\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    preds.extend(torch.argmax(logits, axis=-1).cpu().numpy())\n",
    "    labels.extend(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "acc = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "\n",
    "print(f\" Unfrozen Model - Final Test Results on RTE Validation Set (277 samples)\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "# Save Unfrozen Model\n",
    "torch.save(model.state_dict(), \"roberta_rte_unfrozen.pt\")\n",
    "# --- End of Part 2: Unfrozen Training ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1d3c2f1f-8faf-4228-bcdb-cd43e8796973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Evaluation on RTE Validation Set (277 Samples)\n",
      "Accuracy  : 0.7148\n",
      "Precision : 0.7955\n",
      "Recall    : 0.5344\n",
      "F1 Score  : 0.6393\n"
     ]
    }
   ],
   "source": [
    "# ======================================================\n",
    "#Final Evaluation for Unfrozen RoBERTa on 277 RTE Validation Samples\n",
    "# ======================================================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "preds, labels = [], []\n",
    "\n",
    "# Loop through the full RTE validation set (277 examples)\n",
    "for batch in test_loader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=batch[\"input_ids\"],\n",
    "            attention_mask=batch[\"attention_mask\"],\n",
    "            labels=batch[\"label\"]\n",
    "        )\n",
    "    logits = outputs.logits\n",
    "    preds.extend(torch.argmax(logits, axis=-1).cpu().numpy())\n",
    "    labels.extend(batch[\"label\"].cpu().numpy())\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "acc = accuracy_score(labels, preds)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "\n",
    "# Print final evaluation metrics\n",
    "print(\"Final Evaluation on RTE Validation Set (277 Samples)\")\n",
    "print(f\"Accuracy  : {acc:.4f}\")\n",
    "print(f\"Precision : {precision:.4f}\")\n",
    "print(f\"Recall    : {recall:.4f}\")\n",
    "print(f\"F1 Score  : {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2590032-d947-481e-bd23-63a5ba5d33ca",
   "metadata": {},
   "source": [
    "<h3>Number of trained parameters of each of two finetuned models.</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f187a270-2aa8-464a-af6b-30b642ca14a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frozen Model - Trainable Parameters: 592,130\n",
      "Frozen Model - Total Parameters: 124,647,170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unfrozen Model - Trainable Parameters: 124,647,170\n",
      "Unfrozen Model - Total Parameters: 124,647,170\n"
     ]
    }
   ],
   "source": [
    "# --- Start of Code to Count Trainable Parameters ---\n",
    "def count_trainable_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def count_total_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Example Usage:\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "\n",
    "# Frozen: Freeze all RoBERTa layers\n",
    "for param in model.roberta.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "print(f\"Frozen Model - Trainable Parameters: {count_trainable_parameters(model):,}\")\n",
    "print(f\"Frozen Model - Total Parameters: {count_total_parameters(model):,}\")\n",
    "\n",
    "# Unfrozen: Load new model, all layers trainable\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "print(f\"Unfrozen Model - Trainable Parameters: {count_trainable_parameters(model):,}\")\n",
    "print(f\"Unfrozen Model - Total Parameters: {count_total_parameters(model):,}\")\n",
    "# --- End of Code to Count Trainable Parameters ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa9db4-5571-4b1e-a60f-5cfecc9d4334",
   "metadata": {},
   "source": [
    "<h3>Per class classification report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c3de85a-ce2c-4dd2-812d-cf41933483e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.6615    0.8836    0.7566       146\n",
      "           1     0.7927    0.4962    0.6103       131\n",
      "\n",
      "    accuracy                         0.7004       277\n",
      "   macro avg     0.7271    0.6899    0.6835       277\n",
      "weighted avg     0.7236    0.7004    0.6874       277\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Start of Per-Class Metrics Calculation ---\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# After getting labels and preds from validation or test set:\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(labels, preds, digits=4))\n",
    "# --- End of Per-Class Metrics Calculation ---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02875548-59f3-4bad-ad67-2631268246eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
